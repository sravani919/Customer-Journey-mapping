{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "508921c6-4f60-4c03-90ab-9dc3254c6ab4",
   "metadata": {
    "tags": []
   },
   "source": [
    "After thoroughly researching, I found that **Video-LLaMA2** is the most suitable model for my project for several key reasons:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Comprehensive Multimodal Embedding Capabilities**\n",
    "Video-LLaMA2 excels in processing video, audio, and text modalities together, which aligns perfectly with my project’s requirements. The model’s ability to fuse these diverse inputs into unified embeddings provides rich representations that can be directly applied to tasks like price prediction and recommendation systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Pre-Trained on Large-Scale Datasets**\n",
    "The model has been pre-trained on extensive datasets such as WebVid-2M and LLaVA, covering millions of video-text and image-caption pairs. This robust pre-training ensures that it can handle complex video and audio features effectively, even for diverse and unstructured inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Instruction Fine-Tuning**\n",
    "Video-LLaMA2 has been fine-tuned on datasets like MiniGPT-4, LLaVA, and VideoChat, which enhance its ability to follow human-like instructions and adapt to specific tasks. This fine-tuning is particularly valuable for customizing the model to the project’s goals, such as extracting specific product information or interpreting price-related data in videos.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Flexibility and Customization**\n",
    "The availability of both pre-trained and fine-tuned checkpoints for multiple versions (7B and 13B) allows me to choose the most appropriate configuration based on the project’s complexity and available resources. Additionally, it offers the flexibility to fine-tune the model further, ensuring it can align with niche requirements if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Ability to Process Videos with or Without Audio**\n",
    "The model’s dual branches—Vision-Language (VL) and Audio-Language (AL)—ensure that it can handle a variety of scenarios, such as:\n",
    "- Videos with rich audio content.\n",
    "- Silent videos where visual context alone is critical.\n",
    "\n",
    "This versatility enhances its applicability to diverse datasets and ensures that no data is wasted, regardless of its format.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Cost-Effectiveness with Palmetto Access**\n",
    "By running Video-LLaMA2 on the Palmetto Cluster, I avoid the high costs associated with cloud-based platforms like Google AI Studio. Leveraging Palmetto’s GPU and memory resources allows me to run the model efficiently without budget concerns.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Open-Source and Research-Focused**\n",
    "As an open-source model, Video-LLaMA2 is freely available for research purposes, aligning perfectly with the academic goals of the project. Its strong documentation and active community support also make it easier to set up and troubleshoot.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "In summary, I found Video-LLaMA2 to be the most practical and powerful tool for my project. Its comprehensive multimodal capabilities, cost-effectiveness, and alignment with academic use cases make it the ideal choice for extracting rich embeddings from video and audio content, and ultimately, for achieving the project’s goals. This approach also ensures scalability and flexibility for future refinements.\n",
    "\n",
    "link : https://github.com/DAMO-NLP-SG/Video-LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a580befd-92c8-4e14-a7fc-25b44bd1860d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories set up.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directories for videos, audio, and embeddings\n",
    "videos_dir = './videos/'\n",
    "audio_dir = './audio/'\n",
    "embeddings_dir = './embeddings/'\n",
    "\n",
    "# Create the directories if they do not exist\n",
    "os.makedirs(videos_dir, exist_ok=True)\n",
    "os.makedirs(audio_dir, exist_ok=True)\n",
    "os.makedirs(embeddings_dir, exist_ok=True)\n",
    "\n",
    "print(\"Directories set up.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf0d2a8-9999-4182-8d60-6de3b7eb4685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    https://m.media-amazon.com/images/S/vse-vms-tr...\n",
      "1    https://m.media-amazon.com/images/S/vse-vms-tr...\n",
      "2    https://m.media-amazon.com/images/S/vse-vms-tr...\n",
      "3    https://m.media-amazon.com/images/S/vse-vms-tr...\n",
      "4    https://m.media-amazon.com/images/S/vse-vms-tr...\n",
      "5    https://m.media-amazon.com/images/S/vse-vms-tr...\n",
      "6    https://m.media-amazon.com/images/S/vse-vms-tr...\n",
      "7    https://m.media-amazon.com/images/S/vse-vms-tr...\n",
      "8    https://m.media-amazon.com/images/S/vse-vms-tr...\n",
      "9    https://m.media-amazon.com/images/S/vse-vms-tr...\n",
      "Name: video_urls, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(video_urls_df['video_urls'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc5d2bf6-0f4e-41fc-a861-23a3c84f9b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the URLs and take only the first part\n",
    "video_urls_df['video_urls'] = video_urls_df['video_urls'].str.split(';').str[0].str.strip()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
