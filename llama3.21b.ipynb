{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e576cd-5182-40c2-b067-09a276f0ff4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.11/site-packages (4.48.1)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./.local/lib/python3.11/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.11/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f9f7a5-5951-44a9-b89c-72358ef95dec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./.local/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.11/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.local/lib/python3.11/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./.local/lib/python3.11/site-packages (from accelerate) (0.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.11/site-packages (from accelerate) (0.5.2)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\n",
      "Requirement already satisfied: requests in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78095d4-3d0d-4089-a6fe-30bda1a8d7d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bitsandbytes in ./.local/lib/python3.11/site-packages (0.45.0)\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.11/site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.11/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in ./.local/lib/python3.11/site-packages (from bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch->bitsandbytes) (3.9.0)\n",
      "Requirement already satisfied: networkx in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from torch->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.11/site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from jinja2->torch->bitsandbytes) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc721578-575b-439f-a28e-a8404c1a3150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spati/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Label Map: {'Decision': 0, 'Post-Purchase': 1, 'Awareness': 2, 'Consideration': 3}\n",
      "Added padding token '[PAD]' to tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/home/spati/.local/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/local_scratch/slurm.1723009/ipykernel_4028131/3801307676.py:140: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 02:27, Epoch 6/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.546506</td>\n",
       "      <td>0.455128</td>\n",
       "      <td>0.378295</td>\n",
       "      <td>0.656611</td>\n",
       "      <td>0.455128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.531055</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.830644</td>\n",
       "      <td>0.838041</td>\n",
       "      <td>0.826923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.368570</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.884494</td>\n",
       "      <td>0.875489</td>\n",
       "      <td>0.897436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.270631</td>\n",
       "      <td>0.910256</td>\n",
       "      <td>0.906796</td>\n",
       "      <td>0.906288</td>\n",
       "      <td>0.910256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.298072</td>\n",
       "      <td>0.878205</td>\n",
       "      <td>0.864667</td>\n",
       "      <td>0.865379</td>\n",
       "      <td>0.878205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.229192</td>\n",
       "      <td>0.929487</td>\n",
       "      <td>0.926615</td>\n",
       "      <td>0.924403</td>\n",
       "      <td>0.929487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics: {'eval_loss': 0.22919194400310516, 'eval_accuracy': 0.9294871794871795, 'eval_f1': 0.9266146085114918, 'eval_precision': 0.9244031830238726, 'eval_recall': 0.9294871794871795, 'eval_runtime': 0.3599, 'eval_samples_per_second': 433.511, 'eval_steps_per_second': 5.558, 'epoch': 6.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Decision       1.00      0.97      0.99        70\n",
      "Post-Purchase       0.82      0.82      0.82        28\n",
      "    Awareness       0.93      0.98      0.96        55\n",
      "Consideration       0.00      0.00      0.00         3\n",
      "\n",
      "     accuracy                           0.93       156\n",
      "    macro avg       0.69      0.69      0.69       156\n",
      " weighted avg       0.92      0.93      0.93       156\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "\n",
    "# Ensure correct device usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"dataset_with_topic_labels.csv\"  # Replace with your dataset path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the stage classification logic\n",
    "def classify_stage(row):\n",
    "    overlap = row[\"topics_overlap\"]\n",
    "    topic = row[\"topic_label\"]\n",
    "    sentiment = row[\"sentiment\"]\n",
    "    emotional_tone = row[\"final_consolidated_emotional_tone\"]\n",
    "\n",
    "    if overlap >= 0.2:\n",
    "        return \"Post-Purchase\"\n",
    "    if 0.1 <= overlap < 0.2:\n",
    "        if topic in [\"Connectivity & Portability\", \"Quality & Reviews\"]:\n",
    "            return \"Consideration\"\n",
    "        if topic in [\"Performance & Specifications\", \"Quality & Reviews\"]:\n",
    "            return \"Decision\"\n",
    "    if overlap < 0.1:\n",
    "        if topic in [\"Design & Usability\", \"Protection & Packaging\"]:\n",
    "            return \"Awareness\"\n",
    "        if topic in [\"Performance & Specifications\", \"Quality & Reviews\"]:\n",
    "            return \"Decision\"\n",
    "\n",
    "    if emotional_tone == \"Positive\" and sentiment == \"Positive\":\n",
    "        return \"Post-Purchase\"\n",
    "    elif emotional_tone == \"Neutral\" or sentiment == \"Neutral\":\n",
    "        return \"Consideration\"\n",
    "    elif emotional_tone == \"Negative\" or sentiment == \"Negative\":\n",
    "        return \"Decision\"\n",
    "    elif emotional_tone == \"Mixed\":\n",
    "        return \"Awareness\"\n",
    "\n",
    "    return \"Awareness\"\n",
    "\n",
    "# Apply classification logic\n",
    "df[\"stage\"] = df.apply(classify_stage, axis=1)\n",
    "\n",
    "# Map labels to numerical values\n",
    "label_map = {label: i for i, label in enumerate(df[\"stage\"].unique())}\n",
    "df[\"stage_label\"] = df[\"stage\"].map(label_map)\n",
    "print(\"Label Map:\", label_map)\n",
    "\n",
    "# Split the dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"processed_text\"], df[\"stage_label\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load tokenizer and model\n",
    "#model_name = \"meta-llama/Llama-3.2-3B\"  # Replace with a smaller model for debugging if needed\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "model_name=\"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add a padding token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    print(\"Added padding token '[PAD]' to tokenizer.\")\n",
    "\n",
    "# Load model and resize token embeddings\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label_map)\n",
    ").to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Define a global function for tokenization\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "# Parallel tokenization using the global function\n",
    "def parallel_tokenize(texts):\n",
    "    with multiprocessing.Pool(processes=8) as pool:  # Adjust process count based on resources\n",
    "        tokenized = pool.map(tokenize_function, texts)\n",
    "    return {\n",
    "        key: torch.tensor([item[key] for item in tokenized])\n",
    "        for key in tokenized[0]\n",
    "    }\n",
    "\n",
    "# Perform tokenization\n",
    "train_encodings = parallel_tokenize(train_texts.tolist())  # Convert to list for multiprocessing\n",
    "val_encodings = parallel_tokenize(val_texts.tolist())\n",
    "\n",
    "train_encodings[\"labels\"] = torch.tensor(train_labels.tolist())\n",
    "val_encodings[\"labels\"] = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_dict(train_encodings)\n",
    "val_dataset = Dataset.from_dict(val_encodings)\n",
    "\n",
    "# Define evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "        \"precision\": precision_score(labels, preds, average=\"weighted\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,  # Optimized batch size\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    gradient_accumulation_steps=2,  # Simulate larger batch size\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    optim=\"adamw_bnb_8bit\",  # Efficient optimizer\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Evaluation Metrics:\", metrics)\n",
    "\n",
    "# Analyze performance for a specific class\n",
    "y_true = val_labels.tolist()\n",
    "y_pred = trainer.predict(val_dataset).predictions.argmax(-1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=list(label_map.keys())))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=label_map.keys(), yticklabels=label_map.keys())\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04669984-04b6-48a5-81fe-fdf2f7669ccc",
   "metadata": {},
   "source": [
    "LLAMA 3.2 8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17cf2869-4001-4cce-a220-1334ffe40e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Label Map: {'Decision': 0, 'Post-Purchase': 1, 'Awareness': 2, 'Consideration': 3}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744b9c97714948969dc5c54af584ed4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04141b6a993844ab8c2cf533b486838a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad9a861ba104bfab8ec0560a5b1d4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added padding token '[PAD]' to tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e8dbdc7e1347e19dfa77a10d381f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/928 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee2207d34f648bd95988f78f038d642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c839a18edf3b49ecb46a071735c9cb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e14bb85c1d4fdcafd3dcbb631ffd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00007.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc1e6896c784b93a68e3344ed940eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00007.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9051c926f46e42cdb4627c22a6856fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f284f47b94b941f4929190992868156a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722037d9514f4c779b71fe60a2189916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00007.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db69215aa7a74fd9bb2793251981c335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7171bbdeac3b445b86268da3f3f0fc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00007.safetensors:   0%|          | 0.00/2.57G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbb986903cd4dfc960a42ea99a7876e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at voidful/Llama-3.2-8B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/spati/.local/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/local_scratch/slurm.1723009/ipykernel_4028131/324475308.py:141: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 919, in forward\n    transformer_outputs = self.model(\n                          ^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 589, in forward\n    layer_outputs = decoder_layer(\n                    ^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 348, in forward\n    hidden_states = self.mlp(hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 186, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 432, in forward\n    return F.silu(input, inplace=self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/functional.py\", line 2380, in silu\n    return torch._C._nn.silu(input)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 156.00 MiB is free. Including non-PyTorch memory, this process has 78.93 GiB memory in use. Of the allocated memory 73.35 GiB is allocated by PyTorch, and 139.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 152\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    155\u001b[0m metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2172\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2173\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2174\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2175\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2176\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3675\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[1;32m   3677\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3680\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3681\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:3731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3730\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:212\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[1;32m    211\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parallel_apply(\n\u001b[1;32m    213\u001b[0m         replicas, inputs, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(replicas)]\n\u001b[1;32m    214\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:126\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 126\u001b[0m         output\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m    127\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 919, in forward\n    transformer_outputs = self.model(\n                          ^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 589, in forward\n    layer_outputs = decoder_layer(\n                    ^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 348, in forward\n    hidden_states = self.mlp(hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 186, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 432, in forward\n    return F.silu(input, inplace=self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spati/.local/lib/python3.11/site-packages/torch/nn/functional.py\", line 2380, in silu\n    return torch._C._nn.silu(input)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 156.00 MiB is free. Including non-PyTorch memory, this process has 78.93 GiB memory in use. Of the allocated memory 73.35 GiB is allocated by PyTorch, and 139.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "\n",
    "# Ensure correct device usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"dataset_with_topic_labels.csv\"  # Replace with your dataset path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the stage classification logic\n",
    "def classify_stage(row):\n",
    "    overlap = row[\"topics_overlap\"]\n",
    "    topic = row[\"topic_label\"]\n",
    "    sentiment = row[\"sentiment\"]\n",
    "    emotional_tone = row[\"final_consolidated_emotional_tone\"]\n",
    "\n",
    "    if overlap >= 0.2:\n",
    "        return \"Post-Purchase\"\n",
    "    if 0.1 <= overlap < 0.2:\n",
    "        if topic in [\"Connectivity & Portability\", \"Quality & Reviews\"]:\n",
    "            return \"Consideration\"\n",
    "        if topic in [\"Performance & Specifications\", \"Quality & Reviews\"]:\n",
    "            return \"Decision\"\n",
    "    if overlap < 0.1:\n",
    "        if topic in [\"Design & Usability\", \"Protection & Packaging\"]:\n",
    "            return \"Awareness\"\n",
    "        if topic in [\"Performance & Specifications\", \"Quality & Reviews\"]:\n",
    "            return \"Decision\"\n",
    "\n",
    "    if emotional_tone == \"Positive\" and sentiment == \"Positive\":\n",
    "        return \"Post-Purchase\"\n",
    "    elif emotional_tone == \"Neutral\" or sentiment == \"Neutral\":\n",
    "        return \"Consideration\"\n",
    "    elif emotional_tone == \"Negative\" or sentiment == \"Negative\":\n",
    "        return \"Decision\"\n",
    "    elif emotional_tone == \"Mixed\":\n",
    "        return \"Awareness\"\n",
    "\n",
    "    return \"Awareness\"\n",
    "\n",
    "# Apply classification logic\n",
    "df[\"stage\"] = df.apply(classify_stage, axis=1)\n",
    "\n",
    "# Map labels to numerical values\n",
    "label_map = {label: i for i, label in enumerate(df[\"stage\"].unique())}\n",
    "df[\"stage_label\"] = df[\"stage\"].map(label_map)\n",
    "print(\"Label Map:\", label_map)\n",
    "\n",
    "# Split the dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"processed_text\"], df[\"stage_label\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load tokenizer and model\n",
    "#model_name = \"meta-llama/Llama-3.2-3B\"  # Replace with a smaller model for debugging if needed\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "model_name=\"voidful/Llama-3.2-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Add a padding token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    print(\"Added padding token '[PAD]' to tokenizer.\")\n",
    "\n",
    "# Load model and resize token embeddings\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label_map)\n",
    ").to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Define a global function for tokenization\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "# Parallel tokenization using the global function\n",
    "def parallel_tokenize(texts):\n",
    "    with multiprocessing.Pool(processes=8) as pool:  # Adjust process count based on resources\n",
    "        tokenized = pool.map(tokenize_function, texts)\n",
    "    return {\n",
    "        key: torch.tensor([item[key] for item in tokenized])\n",
    "        for key in tokenized[0]\n",
    "    }\n",
    "\n",
    "# Perform tokenization\n",
    "train_encodings = parallel_tokenize(train_texts.tolist())  # Convert to list for multiprocessing\n",
    "val_encodings = parallel_tokenize(val_texts.tolist())\n",
    "\n",
    "train_encodings[\"labels\"] = torch.tensor(train_labels.tolist())\n",
    "val_encodings[\"labels\"] = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_dict(train_encodings)\n",
    "val_dataset = Dataset.from_dict(val_encodings)\n",
    "\n",
    "# Define evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "        \"precision\": precision_score(labels, preds, average=\"weighted\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,  # Optimized batch size\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    gradient_accumulation_steps=2,  # Simulate larger batch size\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    optim=\"adamw_bnb_8bit\",  # Efficient optimizer\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Evaluation Metrics:\", metrics)\n",
    "\n",
    "# Analyze performance for a specific class\n",
    "y_true = val_labels.tolist()\n",
    "y_pred = trainer.predict(val_dataset).predictions.argmax(-1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=list(label_map.keys())))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=label_map.keys(), yticklabels=label_map.keys())\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908eddc8-a6c7-4b90-930b-3c60248dda76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957933c-3612-436a-a719-360bba1690b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af91240-5b81-4cb0-81b3-daa49e6c5084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532a771-fa20-4602-8604-fc0079fdb6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61e3e7-5c69-4882-9e7a-63e036e60f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
